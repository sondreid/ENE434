---
title: "Lab 7 /Assignment 3 "
output: pdf_document
---
  

```{r, echo = FALSE, include=FALSE}

### Libraries
library(tidyverse)
library(lubridate)
library(magrittr)
library(fpp3)
library(splines)
library(kableExtra)

pv_df= read_csv("http://jmaurit.github.io/analytics/labs/data/pv_df.csv")

pv_df = pv_df %>% arrange(date) %>% mutate(
  cum_cap = cumsum(nameplate)
)
pv_df %<>% mutate(log2_cum_cap = log2(cum_cap),
                 log2_cost_per_kw = log2(cost_per_kw),
                 monthdate =  as.Date(paste(pv_df$year, pv_df$month, "1",sep = "-")))  %>%  
                 filter(cost_per_kw !=0)


######### Models from the lab ###############
 

# Linear model
cap_mod = lm(log2_cum_cap ~ monthdate, data=pv_df) 
new_data = tibble(
  monthdate = seq(ymd("2015-01-01"), ymd("2019-12-31"), by="months")
)

new_data["log2_cum_cap"] = predict(cap_mod, newdata=new_data)

learning_mod1 = lm(log2_cost_per_kw~log2_cum_cap, data=pv_df)

#Polynomial model
learning_mod2 = lm(log2_cost_per_kw~poly(log2_cum_cap,4), data = pv_df)

# Splines 
learning_mod3 = lm(log2_cost_per_kw~bs(log2_cum_cap, knots=c(7, 10, 13, 16)), data=pv_df)


```


## Task 1

We interpret this task in the following way:
For two seperate intervals of time (before 2012, and 2012 onwards), make linear learning curves determined
by the fitted values of a simple linear regression of the log base 2 of the cumulative solar capacity and solar price.

Note that we include 2012 in the 'post' data as we found that the dataset stops at 2014. An exclusion of 2012 would have affected the results substantially,
as there would have been too few datapoints to get a proper result.  

Seperating pre and post 2012 data.
```{r, echo = TRUE, include = TRUE, warning = FALSE}
df_pre_2012 = pv_df %>%  filter(year < 2012)
df_post_2012 = pv_df %>% filter(year >= 2012)

```


Creating the variable cum_cap, which calculates the cumulative sum of nameplate.
```{r, echo = TRUE, include = TRUE, warning = FALSE}
df_pre_2012 = df_pre_2012 %>% arrange(date) %>% mutate(
  cum_cap = cumsum(nameplate)
)

df_post_2012 = df_post_2012 %>% arrange(date) %>% mutate(
  cum_cap = cumsum(nameplate)
)
```


Removing zero values
```{r, echo = TRUE, include = TRUE, warning = FALSE}
df_pre_2012 = df_pre_2012 %>% filter(cost_per_kw != 0)
df_post_2012 = df_post_2012 %>% filter(cost_per_kw != 0)

```


Creating the variables *log2_cum_cap* and *log2_cost_per_kw* 

```{r, echo = TRUE, include = TRUE, warning = FALSE}
df_pre_2012["log2_cum_cap"] = log2(df_pre_2012$cum_cap)
df_pre_2012["log2_cost_per_kw"] = log2(df_pre_2012$cost_per_kw)

df_post_2012["log2_cum_cap"] = log2(df_post_2012$cum_cap)
df_post_2012["log2_cost_per_kw"] = log2(df_post_2012$cost_per_kw)
```

Signficance tests show that the capacity has a statistical signifcant correlation with the solar price.
As we will see later, plotting the regression with observed data confirms this.  

```{r, echo = TRUE, include = TRUE, warning = FALSE}
learning_mod_pre = lm(log2_cost_per_kw~log2_cum_cap, data = df_pre_2012)
summary(learning_mod_pre)

learning_mod_post = lm(log2_cost_per_kw~log2_cum_cap, data = df_post_2012)
summary(learning_mod_post)
```


After splitting and calculating the log base 2 of the cumulative solar capacity and solar cost, we plot the fitted 
values of the linear regression with the observed values.

```{r, echo = TRUE, include = TRUE, warning = FALSE}
df_pre_2012  %>% filter(year < 2012)  %>%  ggplot(aes(x = log2_cum_cap, y = log2_cost_per_kw)) +
  geom_point(alpha=.5, color = "black") +
  geom_smooth(method = "lm", color = "orange") +
  labs(title = "Linear learning curve: -2012 data", 
       x     = "Log base 2 cumulative solar capacity",
       y     = "Log base 2 cost per kw electricity generated") +
  theme_bw()


df_post_2012  %>% filter(year > 2012) %>%  ggplot(aes(x = log2_cum_cap, y = log2_cost_per_kw)) +
  geom_point(alpha=.5, color = "black") +
  geom_smooth(method = "lm", color = "orange") +
  labs(title = "Linear learning curve: 2012- data", 
       x     = "Log base 2 cumulative solar capacity",
       y     = "Log base 2 cost per kw electricity generated") +
  theme_bw() 

```

By using simple linear regression we avoid overfitting, as higher-order polynomials might incurr. While, our estimates does not overfit, 
we do in all likelihood suffer from signifcant bias as we assume a linear relationship between the cumulative solar capacity and solar price.
In reality, a linear relationship is very unlikely and as a result a linear model will give poor predictive performance.

We do not however, capture large amount of the information present within the data. 
As we can see from the plots above, there is signifcant variation and this variation will only be accounted for to a small extent. 


## Task 2
For this task we will explore the relationship between cumulative capacity and solar cost using Local Linear Regression.


We model a new estimate of the learning curve using local regression. As we will see, it captures more of the underlying variation 
in the relationship between solar cost and solar cumulative capacity.

```{r, echo = TRUE, include = TRUE, warning = FALSE}

pv_df_log <- pv_df %>% 
                mutate(log2_cost_per_kw = log2(cost_per_kw),
                      log2_cum_cap = log2(cum_cap),
                      monthdate =  as.Date(paste(pv_df$year, 
                                                 pv_df$month, "1",sep = "-")))  %>% 
                filter(!is.na(cost_per_kw))  %>% 
                filter(!is.na(monthdate))



pv_df  %>% 
  ggplot(aes(log2_cum_cap, log2_cost_per_kw)) + 
  geom_point() +
  geom_smooth(method = "loess", se = FALSE) +
  labs(title = "Fitted values of Local Regression",
       x = "Log base 2 cumulative solar capacity",
       y = "Log base 2 cost per kw electricity generated") +
  theme_bw()

```



Local regression is different from splines in that a set amount of data points is used for each regression.
The regressions are made by a local group of data poins, rather than a set amount of knots. The methods are similar in
that they are a set of regressions conducted on different parts of the dataset.


### Point forecasts

We will now explore whether the improved fit of the local regression model can give a better out-of-sample prediction/forecast.
Assuming a linear evolution in solar capacity, we populate a new data frame with the forecasts of solar cost.

```{r, echo = TRUE, include = TRUE, warning = FALSE}

# Mean solar log cost per month
pv_df_month <- pv_df  %>% 
               group_by(monthdate)  %>% 
               summarise(log2_cost_per_kw = mean(log2_cost_per_kw)) %>% 
               as_tibble()

# Calculating 
loess_fit <-   pv_df %>% loess(log2_cost_per_kw~log2_cum_cap, span=.2, data = .
                                , control=loess.control(surface="direct"))

# New dates as tibble
new_data_fc_loess = tibble(
  monthdate = seq(ymd("2015-01-01"), ymd("2019-12-31"), by="months")
)

# Assuming linear estimate of capacity
new_data_fc_loess["log2_cum_cap"] = predict(cap_mod, newdata = new_data_fc_loess)
# Adding local regression predictions of solar cost
new_data_fc_loess["fc_cost"] = predict(loess_fit, newdata = new_data_fc_loess)


# Plotting forecast of cost
new_data_fc_loess   %>% 
   as_tsibble(index = monthdate)  %>% 
   dplyr::select(fc_cost, monthdate)  %>% 
   ggplot() +
   geom_line(aes(x = monthdate, y = fc_cost, col = "Forecast of solar cost")) +
   geom_point(aes(x = monthdate, y = log2_cost_per_kw, color = "Log base 2 solar cost"), 
   data = pv_df_month) +
   theme_bw() +
   labs(x = "Months",
        y = "Solar cost log base 2",
        title = "Forecast of solar cost (in log base 2)") +
   scale_colour_manual(values = c("orange", "black")) 


```
As we can see from the plot above, the forecast seem to exagerate the evolution in solar cost, approaching 0 by 2020.
This is far from what we actually observe.
We can see that much of the same problems that we experience using splines, simple and polynomial regression occur.
The model does not account for the information contained within the observed data, and rather extrapolates a short term tendency in the data.
This leads to a wild underestimate of actual solar cost.


# Task 3
In this Task we will focus on evaluating predictions of solar cost per month in the extended period of 2015-2019. 

We start of by loading the updated dataset from the **California Solar Initative**, the CSI Working Dataset, and calculating cost per kw and cumulative capacity based on the nameplate ratings of the installed
solar infrastructure. 


```{r, echo = TRUE, include = TRUE, warning = FALSE}
updated_df <- read_csv("lab7_data/WorkingDataSet_5-28-2020.csv")

updated_df["cost_per_kw"] = updated_df$"Total Cost"/updated_df$"Nameplate Rating"
updated_df["cum_sum"] = cumsum(updated_df$"Nameplate Rating")
updated_df["completed_date"] = ymd(updated_df$"First Completed Date")

updated_df <- updated_df %>% filter(cost_per_kw != 0, cum_sum != 0)
updated_df <- updated_df %>% filter(!is.na(cost_per_kw), !is.na(cum_sum))

updated_df["monthdate"] = ymd(paste(year(updated_df$completed_date), 
                                    month(updated_df$completed_date), "1",sep = "-"))

updated_df["log2_cost_per_kw"] = log2(updated_df$cost_per_kw)
updated_df["log2_cum_sum"] = log2(updated_df$cum_sum)

monthly_costs_updated <- updated_df  %>% 
                 group_by(monthdate)  %>% 
                 summarise(log2_cost_per_kw = mean(log2_cost_per_kw)) %>%
                 complete(monthdate = seq.Date(min(ymd("2015-01-01")), 
                                               max(ymd("2019-12-31")), by="month")) 
```

###  Goodness-of-fit model

We create a new test data frame (or tibble) that contains all predicitons of the three models, as well as test data retrieved from the observed data from the CSI data set. 

Aligning the observed values with the predicted values from the linear, polynomial and spline models.
Comparing the values from 2015 to 2020. 
```{r, echo = TRUE, include = TRUE, warning = FALSE}

test_data = tibble(
  monthdate = seq(ymd("2015-01-01"), ymd("2019-12-31"), by="month")
)

test_data <- test_data  %>% 
    mutate(observed_values = (monthly_costs_updated  %>% 
    filter(year(monthdate) > 2014 & year(monthdate) < 2020))$log2_cost_per_kw,
           log2_cum_cap    = predict(cap_mod, newdata = test_data))  

test_data <- test_data  %>%    
    mutate(linear_mod      = predict(learning_mod1, newdata = test_data),
           poly_mod        = predict(learning_mod2, newdata = test_data),
           spline_mod      = predict(learning_mod3, newdata = test_data))  %>% 
    filter(!is.na(observed_values))

```



```{r, echo = TRUE, include = TRUE, warning = FALSE}


linear_resids <- test_data$observed_values-test_data$linear_mod
poly_resids <- test_data$observed_values-test_data$poly_mod
spline_resids <- test_data$observed_values-test_data$spline_mod

linear_table_data <- bind_cols("Model type" =  "Linear", 
                                RMSE = RMSE(linear_resids), 
                                MSE = MSE(linear_resids), 
                                MAE = MAE(linear_resids))
poly_table_data <- bind_cols("Model type" =  "Polynomial",
                              RMSE = RMSE(poly_resids), 
                              MSE = MSE(poly_resids), MAE = MAE(poly_resids))
spline_table_data <- bind_cols("Model type" =  "Spline", 
                              RMSE = RMSE(spline_resids),
                              MSE = MSE(spline_resids), MAE = MAE(spline_resids))

table_data <- bind_rows(linear_table_data, poly_table_data, spline_table_data)  %>% 
              arrange(RMSE)
```

```{r, echo = TRUE, include = TRUE, warning = FALSE}

table_data  %>% kbl(caption = "Prediction accuracy: Solar cost per kw", digits = 3)  %>% 
                kable_classic(full_width = F, html_font = "Times new roman")

```
We have compared the predicted values from the linear, polynomial, and spline models with the observed values from CSI Working dataset. The values are from the period 2015 to 2020. 
We find that the linear model provides the best predicted values.




# Task 4

We will try two predictive models on a new dataset retrieved from Nordpool, the daily power prices of 2020 for the geographic areas in which Nordpool operates.
We opt to model the power prices of Bergen, and start of by loading and cleaning the data.

```{r, echo = TRUE, include = TRUE, warning = FALSE}

# Loading new dataset
elspot_2020 <- read_csv("lab7_data/elspot-prices_2020_daily_nok.csv")
elspot_2021 <- read_csv("lab7_data/elspot-prices_2021_daily_nok.csv")

# Drop first rows
colnames(elspot_2020) <- elspot_2020[2,]
elspot_2020 <- elspot_2020[3:nrow(elspot_2020),]
colnames(elspot_2020)[1] <- "date"

elspot_2020 <- elspot_2020  %>% dplyr::select(date, Bergen)  %>% 
                 mutate(date = lubridate::dmy(date),
                        Bergen = as.numeric(gsub(",", ".", elspot_2020$Bergen)))


# Drop first rows
colnames(elspot_2021) <- elspot_2021[2,]
elspot_2021 <- elspot_2021[3:nrow(elspot_2021),]
colnames(elspot_2021)[1] <- "date"

elspot_2021 <- elspot_2021  %>% dplyr::select(date, Bergen)  %>% 
                 mutate(date = lubridate::dmy(date),
                        Bergen = as.numeric(gsub(",", ".", elspot_2021$Bergen)))

```

## Model 1: Smoothing Spline
The first model we will use is a smoothing regression spline model. 
We opt for the cross validation approach and allow the smooth.spline function from the stats package to determine the smoothing parameter. 

```{r, echo = TRUE, include = TRUE, warning = FALSE}
new_data_smooth_spline <- seq(ymd("2021-01-01"), ymd("2021-03-31"), by="day")

fit_smooth_spline <- smooth.spline(elspot_2020$date, elspot_2020$Bergen, df=16, cv = TRUE)

fitted_smooth_spline_tibble <- tibble(date = elspot_2020$date, fitted_value = fit_smooth_spline$y)

preds <- predict(fit_smooth_spline, newdata = new_data_smooth_spline)

preds_smooth_spline <- tibble(date = new_data_smooth_spline,
                              preds = (predict(fit_smooth_spline, 
                                       newdata = new_data_smooth_spline))$y[1:90])

```

Below is a combined plot of the observed power prices in Bergen, the fitted values using the smoothing spline model and its predicted values
outside the observed range of data. 

```{r, echo = TRUE, include = TRUE, warning = FALSE}
preds_smooth_spline   %>% 
   ggplot() +
   geom_line(aes(x = date, y = preds, color = "Forecast/predictions")) +
   geom_line(aes(x = date, y = Bergen, color = "Observed prices"), data = elspot_2020) +
   geom_line(aes(x = date, y = fitted_value, color = "Fitted values"), 
                                         data = fitted_smooth_spline_tibble) +
   theme_bw() +
   labs(x = "Days",
        y = "Predicted daily power price in Bergen",
        title = "Forecast of Bergen power prices (nok/mWh)") +
   scale_colour_manual(values = c("black","orange",  "#03ecfc")) 

```
The smoothing of the fitted values makes for a rough predictions with low variance but high bias. 
We can see a tendency from looking at the predicted values, that the smoothing spline model suffer from much of the same issues
as higher order polynomial models: namely its exaggeration of the endpoints. The sharp decline in power prices is not confirmed by looking at the actual 2021 power prices. 


```{r, echo = TRUE, include = TRUE, warning = FALSE}
elspot_2021   %>% 
   filter(month(date) < 5)  %>% 
   ggplot() +
   geom_line(aes(x = date, y = Bergen)) +
   theme_bw() +
   labs(x = "Days",
        y = "Predicted daily power price in Bergen",
        title = "Actual daily power prices in Bergen (nok/mWh) in 2021") +
   scale_colour_manual(values = c("#03ecfc")) 

```


## Model 2: Piecewise polynomial

Our second model is a polynomial spline model. We determine the knots manually by consulting the observed power prices levels in 2020. As the knots function does not handle date type variables,
we will use the day number from the start of 2020, as opposed to dates. 


```{r, echo = TRUE, include = TRUE, warning = FALSE}
# Introduce day numbers

elspot_2020["day_nr"]  = seq(1,366)

new_data_spline <- tibble(day_nr = seq(1,90))

# Plot of observed power prices 
elspot_2020   %>% 
   ggplot() +
   geom_line(aes(x = day_nr, y = Bergen)) +
   theme_bw() +
   labs(x = "Days",
        y = "Predicted daily power price in Bergen",
        title = "Actual daily power prices in Bergen (nok/mWh") +
   scale_colour_manual(values = c("#03ecfc")) 

```

We note the sudden spikes and drops in observed power prices and set the knots to act as cutoff points for new regressions.

```{r, echo = TRUE, include = TRUE, warning = FALSE}
fit_splines_elspot <- lm(Bergen~bs(day_nr, 
                                   knots = c(0, 100, 140, 220, 270, 300, 330, 350)), 
                                   data = elspot_2020)


fitted_spline_tibble <- tibble(date = elspot_2020$date, 
                               fitted_value = fit_splines_elspot$fitted)

preds_splines_elspot <- predict(fit_splines_elspot, newdata = new_data_spline)

preds_spline <- tibble(date = seq(ymd("2021-01-01"), ymd("2021-03-31"), by="day"),
                              preds = preds_splines_elspot)



preds_spline   %>% 
   ggplot() +
   geom_line(aes(x = date, y = preds, color = "Forecast/predictions")) +
   geom_line(aes(x = date, y = Bergen, color = "Observed prices"), data = elspot_2020) +
   geom_line(aes(x = date, y = fitted_value, color = "Fitted values"), 
                          data = fitted_spline_tibble) +
   theme_bw() +
   labs(x = "Days",
        y = "Predicted daily power price in Bergen",
        title = "Forecast of Bergen power prices (nok/mWh) using piecewise polynomial model") +
   scale_colour_manual(values = c("black","orange",  "#03ecfc")) 

```


We find that smooth spline (model 1) produces a better fit to the actual data compared to piecewise polynomial (model 2).
The piecewise polynomial model misses much of the variation that we see in the elspot prices. This is because model 2 is highly dependant on the placement of the knots. 
A weakness of this model is that we manually set the knots. Depending on the dataset, this may not be a simple task. 
The difference a variation in knots levels may have, is quite large. We have not explored this, but it may be reasonable to assume that a smooth 
spline model yields more stable results than a piecewise polynomial model.

The piecewise polynomial model produces arguably a better prediction than the smooth spline model, however neither is fit to produce accurate predictions.
This is because both of the models do not concider the entire data set when predicting. This leads the prediction to only follow the trend in the final moment of the dataset.

